{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AMPIS Tutorial.ipynb","provenance":[{"file_id":"16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5","timestamp":1624564909024}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QHnVupBBn9eR"},"source":["# AMPIS tutorial\n","<img src=\"https://github.com/rccohn/AMPIS/blob/master/.github/particles_screenshot.png?raw=true\" width=750>\n","\n","\n","Welcome to ampis! This is the official colab tutorial of ampis. Here, we will go through some basics usage of detectron2, including the following:\n","* Train a model on a dataset of powder images\n","* Run model inference\n","\n","You can make a copy of this tutorial by \"File --> Save a copy in Drive.\"\n","\n","Copyright (c) 2020 Ryan Cohn and Elizabeth Holm. All rights reserved. <br />\n","Licensed under the MIT License (see LICENSE for details) <br />\n","Tutorial written by [Ryan Cohn](https://github.com/rccohn).\n"]},{"cell_type":"markdown","metadata":{"id":"vM54r6jlKTII"},"source":["# Install detectron2 and AMPIS\n","These cells should be run ONCE to install the required libraries. After the initial installation of detectron2 and AMPIS, the runtime must be restarted (Runtime --> Restart runtime). Otherwise the libraries will not be able to be imported.\n","\n","The last line, exit(), will end the current runtime. You may get a warning from colab that the session 'crashed' for an unknown reason. This is likely that reason. Just proceed as usual."]},{"cell_type":"code","metadata":{"id":"FsePPpwZSmqt"},"source":["try: # if libraries are not installed, then install them. Otherwise, proceed normally.\n","  import ampis\n","  import detectron2\n","\n","except ImportError:\n","  # detectron2  installation\n","  !pip install pyyaml==5.1\n","  # workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https://github.com/facebookresearch/detectron2/issues/3158)\n","  !pip uninstall -y torch torchvision torchtext\n","  !pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","  # install detectron2 that matches pytorch 1.8\n","  # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","  !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n","  # exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime\n","\n","  # get AMPIS\n","  # note we clone the repo instead of simply installing from git\n","  # so that the example data is easier to work with\n","  !git clone https://github.com/rccohn/AMPIS.git\n","\n","  # install ampis\n","  # note after installation you must restart the runtime\n","  # otherwise you will run into \"No module named 'ampis'\"\n","  !pip install -e AMPIS\n","  exit(0) # restarts runtime\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVcf5lfW6OSu"},"source":["## Verify installation\n","If everything is installed correctly, this cell should run without errors"]},{"cell_type":"code","metadata":{"id":"n4Zsy3-Px_3r"},"source":["# check pytorch installation: \n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","assert torch.__version__.startswith(\"1.8\")   # please manually install torch 1.8 if Colab changes its default version\n","\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# ampis\n","import ampis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mh6z3fvy8NJg"},"source":["# Loading data and model training\n","This example will take you through the process of training a model to segment powder particles and visualizing the predictions."]},{"cell_type":"markdown","metadata":{"id":"z_ELdgey864a"},"source":["## Module imports"]},{"cell_type":"code","metadata":{"id":"Vw8lUeys7UjB"},"source":["import cv2\n","import numpy as np\n","import os\n","from pathlib import Path\n","import pickle\n","import sys\n","\n","## detectron2\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.data import (\n","    DatasetCatalog,\n","    MetadataCatalog,\n",")\n","from detectron2.engine import DefaultTrainer, DefaultPredictor\n","\n","from ampis import data_utils, visualize\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWkNlATcTPA_"},"source":["## Labeling Data\n","\n","The recommended tool for labeling is the [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/) To save you the trouble of having to annotate data yourself, existing labels are available under ampis/examples/powder/data/via_2.0.8. For labeling new datasets we follow the same process described in the 'balloon example' [here](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46).\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l8Dk2-O3U1Av"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Q6x9FCF9U1Jb"},"source":["## Loading Data\n","We need to specify the path to the VIA annotation files.\n","The paths to individual images, and all annotation data are stored in these JSON files.\n","\n","In this tutorial we will focus on segmenting powder particles. AMPIS also includes an example for detecting satellites. The process for training models for powder particles and satellites is identical.\n"]},{"cell_type":"code","metadata":{"id":"pd6NePmj9H8j"},"source":["EXPERIMENT_NAME = 'particle' # can be 'particle' or 'satellite'\n","root = Path('AMPIS','examples','powder') # path to folder  containing labels\n","json_path_train = Path(root,'data','via_2.0.8/', f'via_powder_{EXPERIMENT_NAME}_masks_training.json')  # path to training data\n","json_path_val = Path(root,'data','via_2.0.8/', f'via_powder_{EXPERIMENT_NAME}_masks_validation.json')  # path to validation data\n","\n","assert json_path_train.is_file(), 'training file not found!'\n","assert json_path_val.is_file(), 'validation file not found!'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeZVI9xwV5uO"},"source":["### Registration\n","Detectron2 requires that datasets be registered for later use.\n","Registration stores the name of the dataset and a function that can be used to retrieve the image paths and labels in a format that the model can use.\n","\n","To make registration easier, AMPIS provides the get_ddicts() function, which can be used to load data from a VIA annotation file into a format that detectron2 can work with."]},{"cell_type":"code","metadata":{"id":"5exT670TUlDA"},"source":["DatasetCatalog.clear()  # resets catalog, helps prevent errors from running cells multiple times\n","\n","# store names of datasets that will be registered for easier access later\n","dataset_train = f'{EXPERIMENT_NAME}_Train'\n","dataset_valid = f'{EXPERIMENT_NAME}_Val'\n","\n","# register the training dataset\n","DatasetCatalog.register(dataset_train, \n","                        lambda f = json_path_train: data_utils.get_ddicts(label_fmt='via2',  # annotations generated from vgg image annotator\n","                                                                          im_root=f,  # path to the training data json file\n","                                                                          dataset_class='Train'))  # indicates this is training data\n","\n","# register the validation dataset. Same exact process as above\n","DatasetCatalog.register(dataset_valid, \n","                        lambda f = json_path_val: data_utils.get_ddicts(label_fmt='via2',  # annotations generated from vgg image annotator\n","                                                                        im_root=f,  # path to validation data json file\n","                                                                        dataset_class='Validation'))  # indicates this is validation data\n","print(f'Registered Datasets: {list(DatasetCatalog.data.keys())}')\n","\n","## There is also a metadata catalog, which stores the class names.\n","for d in [dataset_train, dataset_valid]:\n","    MetadataCatalog.get(d).set(**{'thing_classes': [EXPERIMENT_NAME]})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrTiyYu3X4Ah"},"source":["### Visualize labeled data\n","This allows for verification that the labels loaded correctly and make sense\n","\n","\n","Also, this is a great chance to admire my hand-drawn labels, which took a really really long time to make!"]},{"cell_type":"markdown","metadata":{"id":"YPqbJDUMUU1Q"},"source":["#### Training images\n","Since there are more images (especially for satellites) we will only view a subset"]},{"cell_type":"code","metadata":{"id":"tVJbQkhBYlLL"},"source":["np.random.seed(42960)\n","for i in np.random.choice(DatasetCatalog.get(dataset_train), 3, replace=False):\n","    visualize.display_ddicts(i, None, dataset_train, suppress_labels=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iv7OCod1Yot_"},"source":["#### Validation images"]},{"cell_type":"code","metadata":{"id":"WAvkve2C-IoE"},"source":["for i in DatasetCatalog.get(dataset_valid):\n","    visualize.display_ddicts(i, None, dataset_valid, suppress_labels=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwXL6btmZs1k"},"source":["## Model Configuration\n","This is where we specify the directory where the outputs are saved, various hyperparameters for the model, and more.\n","\n","The complete set of configurations is listed in the [detectron2 documentation](https://detectron2.readthedocs.io/en/latest/modules/config.html#config-references), but some of the more relevant ones are specified here."]},{"cell_type":"code","metadata":{"id":"FpNxc9F1ZyOQ"},"source":["cfg = get_cfg() # initialize cfg object\n","cfg.merge_from_file(model_zoo.get_config_file('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'))  # load default parameters for Mask R-CNN\n","cfg.INPUT.MASK_FORMAT = 'polygon'  # masks generated in VGG image annotator are polygons\n","cfg.DATASETS.TRAIN = (dataset_train,)  # dataset used for training model\n","cfg.DATASETS.TEST = (dataset_train, dataset_valid)  # we will look at the predictions on both sets after training\n","cfg.SOLVER.IMS_PER_BATCH = 1 # number of images per batch (across all machines)\n","cfg.SOLVER.CHECKPOINT_PERIOD = 400  # number of iterations after which to save model checkpoints\n","cfg.MODEL.DEVICE='cuda'  # 'cpu' to force model to run on cpu, 'cuda' if you have a compatible gpu\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # Since we are training separate models for particles and satellites there is only one class output\n","cfg.TEST.DETECTIONS_PER_IMAGE = 400 if EXPERIMENT_NAME == 'particle' else 150  # maximum number of instances that can be detected in an image (this is fixed in mask r-cnn)\n","cfg.SOLVER.MAX_ITER = 2000  # maximum number of iterations to run during training\n","  # Increasing this may improve the training results, but will take longer to run (especially without a gpu!)\n","\n","# model weights will be downloaded if they are not present\n","weights_path = Path('AMPIS','models','model_final_f10217.pkl')\n","if weights_path.is_file():\n","    print('Using locally stored weights: {}'.format(weights_path))\n","else:\n","    weights_path = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","    print('Weights not found, weights will be downloaded from source: {}'.format(weights_path))\n","cfg.MODEL.WEIGHTs = str(weights_path)\n","cfg.OUTPUT_DIR = str(Path(f'{EXPERIMENT_NAME}_output'))\n","# make the output directory\n","os.makedirs(Path(cfg.OUTPUT_DIR), exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55j5uxjudQxd"},"source":["## Training\n","\n","After all that setup, we're finally ready to train the model!\n","With all of the parameters already specified in the config specified above, running training is very easy"]},{"cell_type":"code","metadata":{"id":"xN5t2MC-dvbk"},"source":["# note this cell generates a huge wall of text\n","trainer = DefaultTrainer(cfg)  # create trainer object from cfg\n","trainer.resume_or_load(resume=False)  # start training from iteration 0\n","trainer.train()  # train the model!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nhdl-zv6fJzC"},"source":["## Visualizing model predictions"]},{"cell_type":"code","metadata":{"id":"Hdo7jh5ze95W"},"source":["# load the weights of the model we want to use \n","model_checkpoints = sorted(Path(cfg.OUTPUT_DIR).glob('*.pth'))  # paths to weights saved druing training\n","cfg.DATASETS.TEST = (dataset_train, dataset_valid)  # predictor requires this field to not be empty\n","cfg.MODEL.WEIGHTS = str(model_checkpoints[-1])  # use the last model checkpoint saved during training. If you want to see the performance of other checkpoints you can select a different index from model_checkpoints.\n","predictor = DefaultPredictor(cfg)  # create predictor object"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QEHv_AFHgQJw"},"source":["### Single image\n","We can run the model on any image.\n","Note the image does not have to already be in a registered dataset.\n"]},{"cell_type":"code","metadata":{"id":"K74TgRl0fe3M"},"source":["img_path = Path(root, 'data','images_png','Sc1Tile_001-005-000_0-000.png')\n","img = cv2.imread(str(img_path))\n","outs = predictor(img)\n","data_utils.format_outputs(img_path, dataset='test', pred=outs)\n","visualize.display_ddicts(ddict=outs,  # predictions to display\n","                                 outpath=None, dataset='Test',  # don't save figure\n","                                 gt=False,  # specifies format as model predictions\n","                                img_path=img_path)  # path to image\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iavuw7Sagfpt"},"source":["### All images in training and validation sets\n","We will save the results for later use."]},{"cell_type":"code","metadata":{"id":"6wUbLUmjfwhL"},"source":["results = []\n","for ds in cfg.DATASETS.TEST:\n","    print(f'Dataset: {ds}')\n","    for dd in DatasetCatalog.get(ds):\n","        print(f'\\tFile: {dd[\"file_name\"]}')\n","        img = cv2.imread(dd['file_name'])  # load image\n","        outs = predictor(img)  # run inference on image\n","        \n","        # format results for visualization and store for later\n","        # note the use of format_outputs(), which ensures that the data is stored correctly for later\n","        results.append(data_utils.format_outputs(dd['file_name'], ds, outs))\n","\n","        # visualize results\n","        visualize.display_ddicts(outs, None, ds, gt=False, img_path=dd['file_name'])\n","\n","# save to disk\n","prediction_save_path = Path(f'{EXPERIMENT_NAME}-results.pickle')\n","with open(prediction_save_path, 'wb') as f:\n","    pickle.dump(results, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhK9Q_jgo4AB"},"source":["### Downloading the results (optional)\n","Uncomment and run the following cell to download the predictions and model weights to your own computer."]},{"cell_type":"code","metadata":{"id":"495OfKH_oQHy"},"source":["# from google.colab.files import download\n","# download(model_checkpoints[-1]) # model weights (large file, may take awhile)\n","# download(prediction_save_path) # predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbivJGVErO8i"},"source":["# Evaluating the results\n","This section can be run independently of model training.\n","\n","Note you must still run the \"Install detectron2 and AMPIS\" section above to install the required libraries."]},{"cell_type":"markdown","metadata":{"id":"Yybo9EgadXet"},"source":["## Module imports"]},{"cell_type":"code","metadata":{"id":"f-Oq2Lh-r34c"},"source":["# some of these are redundant, but allows this section to be run independently of the training section\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from pathlib import Path\n","import pickle\n","import seaborn as sns\n","import skimage.io\n","\n","from ampis import analyze, data_utils\n","from ampis.applications import powder\n","from ampis.structures import InstanceSet\n","from ampis.visualize import display_iset\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXQMy4K0lrWz"},"source":["## Loading data\n","\n","For model evaluation we need both the ground truth and predicted labels.\n","\n","If you ran the model to generate your own predicted labels, you may change the path to use your own predicted labels. However, by default we will use the predicted labels from the original study."]},{"cell_type":"code","metadata":{"id":"pvKJ8UF-lm5E"},"source":["## load ground truth labels\n","root = Path('AMPIS','examples','powder')\n","via_path = Path(root, 'data','via_2.0.8')\n","\n","particles_gt_path_train = via_path / 'via_powder_particle_masks_training.json'\n","particles_gt_path_valid = via_path / 'via_powder_particle_masks_validation.json'\n","\n","satellites_gt_path_train = via_path / 'via_powder_satellite_masks_training.json'\n","satellites_gt_path_valid = via_path / 'via_powder_satellite_masks_validation.json'\n","\n","for path in [particles_gt_path_train, particles_gt_path_valid, satellites_gt_path_train, satellites_gt_path_valid]:\n","    assert path.is_file(), f'File not found : {path}'\n","\n","# note that get_ddicts() loads the data in the standard detectron2 format\n","particles_gt_dd_train = data_utils.get_ddicts('via2', particles_gt_path_train, dataset_class='train')\n","particles_gt_dd_valid = data_utils.get_ddicts('via2', particles_gt_path_valid, dataset_class='validation')\n","\n","satellites_gt_dd_train = data_utils.get_ddicts('via2', satellites_gt_path_train, dataset_class='train')\n","satellites_gt_dd_valid = data_utils.get_ddicts('via2', satellites_gt_path_valid, dataset_class='validation')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EllcyJQgntLX"},"source":["# load predicted labels\n","particles_path = Path(root, 'data','sample_particle_outputs.pickle')\n","assert particles_path.is_file()\n","\n","satellites_path = Path(root, 'data','sample_satellite_outputs.pickle')\n","assert satellites_path.is_file()\n","\n","with open(particles_path, 'rb') as f:\n","    particle_pred = pickle.load(f)\n","\n","with open(satellites_path, 'rb') as f:\n","    satellites_pred = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-g_i2myogMt"},"source":["### Converting from \"data-dicts\" to InstanceSet objects\n","\n","AMPIS uses the \"InstanceSet\" class for visualizing and evaluating the results, as well as for the final sample characterization. The InstanceSet objects provides functionality that is much more convenient to work with compared to the data-dictionary format used in detectron2.\n","\n","The following cell converts the ground-truth and predicted annotations for each image to a list of InstanceSet objects. \n","\n","For the ground truth data we start with the training data, and then include the validation data at the end of the list. This is more compact than loading the training and validation data into separate lists."]},{"cell_type":"code","metadata":{"id":"gRkpRE_yoOhq"},"source":["# Ground truth instance sets\n","\n","iset_particles_gt = [InstanceSet().read_from_ddict(x,   # data \n","                                                   inplace=False  # returns the set so it can be added to the list\n","                                                  ) for x in particles_gt_dd_train]\n","\n","# instead of creating a separate list, we add the validation results to the training ones to make it easier later\n","iset_particles_gt.extend([InstanceSet().read_from_ddict(x, inplace=False) for x in particles_gt_dd_valid])\n","\n","iset_satellites_gt = [InstanceSet().read_from_ddict(x, inplace=False) for x in satellites_gt_dd_train]\n","iset_satellites_gt.extend([InstanceSet().read_from_ddict(x, inplace=False) for x in satellites_gt_dd_valid])\n","\n","# Predicted instance sets\n","iset_particles_pred = [InstanceSet().read_from_model_out(x, inplace=False) for x in particle_pred]\n","iset_satellites_pred = [InstanceSet().read_from_model_out(x, inplace=False) for x in satellites_pred]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZzKfCGWupTCW"},"source":["### Matching the order of ground truth and predicted data\n","The ordering of the loaded data might be inconsistent. The analysis is easier when iset_satellites_gt and iset_particles_pred \n","We want to rearrange the order of the instance sets in the ground truth and predicted lists so that the files are in the same order. This will make it easier later. In the following cell we match the ordering of the predicted instances to match that of the ground truth instances."]},{"cell_type":"code","metadata":{"id":"fbCj_dL9pGza"},"source":["iset_particles_gt, iset_particles_pred = analyze.align_instance_sets(iset_particles_gt, iset_particles_pred)\n","iset_satellites_gt, iset_satellites_pred = analyze.align_instance_sets(iset_satellites_gt, iset_satellites_pred)\n","\n","# to verify that the filenames match we can print them out\n","for i, (gt, pred) in enumerate(zip(iset_particles_gt, iset_particles_pred)):\n","    pred.HFW = gt.HFW # predicted instances don't have specified HFW yet\n","    pred.HFW_units = gt.HFW_units\n","    assert Path(gt.filepath).name == Path(pred.filepath).name\n","    print(f'index {i}:  gt filename: {Path(gt.filepath).name}\\t pred filename: {Path(pred.filepath).name}')\n","    pred.filepath = gt.filepath # the original AMPIS example had a different file organization than the colab\n","    # version. Because VIA uses relative paths it is not affected by this.\n","    # To prevent 'file not found' errors, update the file path for the images in the predicted instances\n","    # to match the true location of the file in Colab.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FuphOq-3uouG"},"source":["## Visualizing predicted labels on validation image"]},{"cell_type":"code","metadata":{"id":"cu1Tm6Dkr9om"},"source":["# for now, the image has to be loaded separately\n","# updating the function to automatically find the image is on the to-do list\n","# it also appears the image must be loaded as an RGB image. This might have to do with the package versions on colab...\n","img = skimage.io.imread(iset_particles_pred[-1].filepath)\n","img = skimage.color.gray2rgb(img)\n","display_iset(img, iset_particles_pred[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXqQZPT9vI9_"},"source":["## Compute performance metrics\n","\n","The standard metrics for evaluation provided in AMPIS are the detection and segmentation precision and recall. These results are described in detail in Section 2.3 of [the original paper](https://arxiv.org/abs/2101.01585). \n","\n","Basically, the ground truth and predicted instances are overlaid to determine which predicted instances correctly correspond to a ground truth instance by a criteria of minimum overlap. Detection precision and recall indicate how many instances were correctly matched. Segmentation precision and recall measure how *well* the predicted and ground-truth masks agree.\n","\n","These metrics are computed and returned as a dictionary in the function det_seg_scores()\n","\n","To keep things simple we will only look at powder particles for now. The process for evaluating the satellite masks is exactly the same."]},{"cell_type":"code","metadata":{"id":"dY0oGiRquMQR"},"source":["dss_particles = [analyze.det_seg_scores(gt, pred, size=gt.instances.image_size) \n","                 for gt, pred in zip(iset_particles_gt, iset_particles_pred)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gHEnielycoP"},"source":["### Visualize detection performance\n","\n","\n","In the visualiaztion, true positive masks (gt matches pred) are shown in purple, false positive masks (unmatched predicted masks) are shown in blue, and false negatives (unmatched ground truth instances) are shown in red."]},{"cell_type":"code","metadata":{"id":"LBcs3KZryaIw"},"source":["gt = iset_particles_gt[-3]\n","pred = iset_particles_pred[-3]\n","iset = gt\n","iset_det, colormap = analyze.det_perf_iset(gt, pred) # the results are returned as an InstanceSet for easy visualization\n","img = skimage.color.gray2rgb(skimage.io.imread(iset.filepath)) # again, in colab we have to convert to rgb for some reason\n","display_iset(img, iset=iset_det)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xis3Utqn05b6"},"source":["We can also plot the quantitative values for the detection precision and recall."]},{"cell_type":"code","metadata":{"id":"559UQdC3zVIt"},"source":["labels = []\n","counts = {'train': 0, 'validation': 0}\n","\n","# the filenames are not helpful, so we will map them to labels ie ('Train 1', 'Train 2', 'Validation 1', etc)\n","for iset in iset_particles_gt:\n","    counts[iset.dataset_class] += 1\n","    labels.append('{} {}'.format(iset.dataset_class, counts[iset.dataset_class]))\n","\n","# x values are arbitrary, we just want 2 values, 1 for precision, 2 for recall\n","x=[*([1] * len(labels)), *([2] * len(labels))]\n","# y values are the bar heights\n","scores = [*[x['det_precision'] for x in dss_particles],\n","     *[x['det_recall'] for x in dss_particles]]\n","\n","# since we are plotting precision and recall on the same plot we need 2 sets of labels\n","if len(labels) < len(x): # prevent length from changing if cell is re-run\n","  labels = labels * 2\n","print('x: ', x)\n","print('y: ', [np.round(x, decimals=2) for x in scores])\n","print('labels: ', labels)\n","\n","fig, ax = plt.subplots(figsize=(6,3), dpi=150)\n","sns.barplot(x=x, y=scores, hue=labels, ax=ax)\n","\n","ax.legend(bbox_to_anchor=(1,1))\n","ax.set_ylabel('detection score')\n","ax.set_xticklabels(['precision','recall'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4W-dQuk1XUc"},"source":["### Visualize segmentation performance\n","Similar to above, true positives, false positives, and false negatives are shown in purple, blue, and red. However, for the segmentation scores, true positives are pixels included in both the predicted segmentation match and the corresponding ground truth mask it matches to. False positives are pixels included in the predicted mask but not the ground truth mask. False negatives are pixels included in the ground truth masks that were missed by the model and are not included in the predicted masks.\n","\n"]},{"cell_type":"code","metadata":{"id":"oRgqnYix12kw"},"source":["iset_seg, (colors, color_labels) = analyze.seg_perf_iset(gt, pred,)\n","display_iset(img, iset=iset_seg, apply_correction=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnTVh2k56lo8"},"source":["Unlike detection precision and recall, where each mask contributes 1 value for true/false positives or false negatives, the segmentation precision and recall contain true/false positives or false negatives for every individual pixel in the mask. Thus, instead of getting a single value for an image, we get a distribution of values, which can be displayed with the boxplot."]},{"cell_type":"code","metadata":{"id":"Ajk2k3r32NTg"},"source":["import pandas as pd # this makes setting up the box plots easier\n","# y values are the bar heights\n","scores = [*[x['seg_precision'] for x in dss_particles],\n","     *[x['seg_recall'] for x in dss_particles]]\n","\n","# due to the way seaborn handles data it is easier to move the data into a dataframe before generating the boxplot\n","dfs = []\n","for score, label, xi in zip(scores, labels, x):\n","  df_sub = pd.DataFrame({'score': score})\n","  df_sub['label'] = label\n","  df_sub['x'] = xi\n","  dfs.append(df_sub)\n","df = pd.concat(dfs)\n","\n","\n","\n","fig, ax = plt.subplots(figsize=(6,3), dpi=150)\n","sns.boxplot(x='x', y='score', hue='label', data=df, ax=ax)\n","ax.legend(bbox_to_anchor=(1,1))\n","ax.set_ylabel('segmentation score')\n","ax.set_xticklabels(['precision','recall'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGGhPVwW91hc"},"source":["# Sample characterization\n","Now for the fun part! We can actually generate real physical measurements of samples from the segmentation masks generated before!"]},{"cell_type":"markdown","metadata":{"id":"rNxETAqk--HX"},"source":["## Size distribution\n","Once we have the masks it is pretty trivial to compute various properties. With binary masks we can use [skimage regionprops](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops), which provides many convenient measurements out of the box. If there are any additional measurements you need, you can also access the masks directly and define your own methods. "]},{"cell_type":"code","metadata":{"id":"_de_sEpw7NW6"},"source":["# this takes ~30 seconds to run in colab\n","for iset in [*iset_particles_gt, *iset_particles_pred]:\n","    if iset.rprops is None:  # avoid re-computing regionprops if cell has already been run\n","        iset.compute_rprops()  # since rprops requires the masks to be uncompressed, this takes a bit longer to run\n","iset_particles_pred[-1].rprops.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oF-_TpVQ_3jP"},"source":["psd_results_gt = powder.psd(iset_particles_gt, plot=False, return_results=True)\n","psd_results_pred = powder.psd(iset_particles_pred, plot=False, return_results=True)\n","\n","# make sure we are plotting the same thing...\n","assert psd_results_gt['x_label'] == psd_results_pred['x_label']\n","assert psd_results_gt['y_label'] == psd_results_pred['y_label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EMbk11DFrX8"},"source":["fig, ax = plt.subplots()\n","ax.plot(psd_results_gt['x'], psd_results_gt['y'], '--k', label='ground truth')\n","ax.plot(psd_results_pred['x'], psd_results_pred['y'], '-.m', label='predicted')\n","leg = ax.legend()\n","ax.set_xlabel(psd_results_gt['x_label'])\n","ax.set_ylabel(psd_results_gt['y_label'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LjJIk4oGZl8"},"source":["#### NOTE\n","The sample dataset does not have many particles. Thus, the distributions don't look really smooth. To increase the number of particles, the results include both masks from the training and validation sets. Because it includes training data, the agreement between ground truth and predicted distributions appears to be really good, but note that this isn't a fair comparison! This is just for demonstration purposes."]},{"cell_type":"markdown","metadata":{"id":"T-jXUIfTG1Ba"},"source":["## Satellite Measurements\n","The claim to fame of the original study! This is first way of generating consistent, reproducible measurements for the fraction of satellited particles in powder samples.\n","\n","The process here is fairly straightforward. We have masks for powder particles and masks for satellites. To match the satellites to their corresponding particles, we simply overlay the masks and look for intersections. Then, it is trivial to count the number of particles containing satellites. "]},{"cell_type":"markdown","metadata":{"id":"vwD8mobwHfyI"},"source":["We have more labeled satellite images than particle images. We only want to keep images that have labels for both particles and satellites.\n","To help with the implementation, we can combine the masks for particles and satellites in the PowderSatelliteImage class"]},{"cell_type":"code","metadata":{"id":"_Fc8rfc1FWd9"},"source":["# as well as aligning the order of files, the align_instance_sets function can also be used to remove extra \n","# files that are present in  one dataset (in this case, labeled satellite images) but not the other (particles)\n","\n","# ensure file ordering is the same, remove excess files for satellite labels\n","iset_particles_gt_ss, iset_satellites_gt_ss = analyze.align_instance_sets(iset_particles_gt, iset_satellites_gt)\n","iset_particles_pred_ss, iset_satellites_pred_ss = analyze.align_instance_sets(iset_particles_pred, iset_satellites_pred)\n","\n","# The PowderSatelliteImage class contains InstanceSet objects for both masks and satellites for the same image \n","psi_gt = []\n","psi_pred = []\n","for pg, pp, sg, sp in zip(iset_particles_gt_ss, iset_particles_pred_ss, iset_satellites_gt_ss, iset_satellites_pred_ss):\n","    files = [Path(x).name for x in [pg.filepath, pp.filepath, sg.filepath, sp.filepath]]\n","    assert all([x == files[0] for x in files])  # the files are in the same order and there are no excess files\n","    psi_gt.append(powder.PowderSatelliteImage(particles=pg, satellites=sg))\n","    psi_pred.append(powder.PowderSatelliteImage(particles=pp, satellites=sp))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"haf4fmyYICgH"},"source":["# compute_matches() finds which satellites belong to which particles in a given image\n","for gt, pred in zip(psi_gt, psi_pred):\n","    for psi in [gt, pred]:\n","        psi.compute_matches()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sws3a18fIXWy"},"source":["### Visualizing satellited particles"]},{"cell_type":"code","metadata":{"id":"8l405JAXIM8Y"},"source":["gt = psi_gt[0]\n","pred = psi_pred[0]\n","\n","np.random.seed(887890)\n","gt_idx = np.random.choice(list(gt.matches['match_pairs'].keys()), 3)\n","pred_idx = np.random.choice(list(pred.matches['match_pairs'].keys()), 3)\n","\n","fig, ax = plt.subplots(2,3)\n","\n","\n","for i, (g, p) in enumerate(zip(gt_idx, pred_idx)):\n","    gt.visualize_particle_with_satellites(g, ax[0, i])\n","    pred.visualize_particle_with_satellites(p, ax[1, i])\n","ax[0,0].set_title('ground truth')\n","ax[1,0].set_title('predicted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"glXiG_Z3IhRk"},"source":["### Quantitative analysis of satellites\n","Again, note that the predicted results include predictions on the training images to increase the sample size. For real experiments, make sure you only use images that the model has not been trained on!\n","\n","The summary is printed by the function."]},{"cell_type":"code","metadata":{"id":"17ft9PD9IayP"},"source":["print('results')\n","results_gt = powder.satellite_measurements(psi_gt, print_summary=True, output_dict=True)\n","print('\\n\\n')\n","print('predicted results')\n","results_pred = powder.satellite_measurements(psi_pred, True, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PxzU79Z-JIbi"},"source":["The results are also returned as a dictionary so that they can be processed programatically."]},{"cell_type":"code","metadata":{"id":"B08RDXkQI_5o"},"source":["results_pred"],"execution_count":null,"outputs":[]}]}